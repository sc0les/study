{\rtf1\ansi\ansicpg1252\cocoartf2638
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\margl1440\margr1440\vieww30820\viewh20000\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 (base) samuelcolelli@Samuels-MBP ~ % conda activate big_data_project\
(big_data_project) samuelcolelli@Samuels-MBP ~ % ssh -i swc_bstn_keys.pem hadoop@ec2-34-226-155-100.compute-1.amazonaws.com \
Warning: Identity file swc_bstn_keys.pem not accessible: No such file or directory.\
Last login: Sun Nov 13 20:29:46 2022\
\
       __|  __|_  )\
       _|  (     /   Amazon Linux AMI\
      ___|\\___|___|\
\
https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\
65 package(s) needed for security, out of 92 available\
Run "sudo yum update" to apply all updates.\
                                                                    \
EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR    \
E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R   \
EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R \
  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R\
  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R\
  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R \
  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR   \
  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R  \
  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R\
  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R\
EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R\
E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R\
EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR\
                                                                    \
[hadoop@ip-172-31-85-183 ~]$ hadoop distcp s3://brainstation-dsft/eng_1M_1gram.csv /user/hadoop/eng_1M_1gram\
22/11/13 20:32:01 INFO tools.DistCp: Input Options: DistCpOptions\{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=100, sslConfigurationFile='null', copyStrategy='uniformsize', preserveStatus=[], preserveRawXattrs=false, atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3://brainstation-dsft/eng_1M_1gram.csv], targetPath=/user/hadoop/eng_1M_1gram, targetPathExists=false, filtersFile='null'\}\
22/11/13 20:32:01 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-85-183.ec2.internal/172.31.85.183:8032\
22/11/13 20:32:04 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 1; dirCnt = 0\
22/11/13 20:32:04 INFO tools.SimpleCopyListing: Build file listing completed.\
22/11/13 20:32:04 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\
22/11/13 20:32:04 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\
22/11/13 20:32:04 INFO tools.DistCp: Number of paths in the copy list: 1\
22/11/13 20:32:04 INFO tools.DistCp: Number of paths in the copy list: 1\
22/11/13 20:32:04 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-85-183.ec2.internal/172.31.85.183:8032\
22/11/13 20:32:04 INFO mapreduce.JobSubmitter: number of splits:1\
22/11/13 20:32:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1668371260186_0001\
22/11/13 20:32:05 INFO impl.YarnClientImpl: Submitted application application_1668371260186_0001\
22/11/13 20:32:05 INFO mapreduce.Job: The url to track the job: http://ip-172-31-85-183.ec2.internal:20888/proxy/application_1668371260186_0001/\
22/11/13 20:32:05 INFO tools.DistCp: DistCp job-id: job_1668371260186_0001\
22/11/13 20:32:05 INFO mapreduce.Job: Running job: job_1668371260186_0001\
22/11/13 20:32:11 INFO mapreduce.Job: Job job_1668371260186_0001 running in uber mode : false\
22/11/13 20:32:11 INFO mapreduce.Job:  map 0% reduce 0%\
22/11/13 20:32:27 INFO mapreduce.Job:  map 100% reduce 0%\
22/11/13 20:33:11 INFO mapreduce.Job: Job job_1668371260186_0001 completed successfully\
22/11/13 20:33:11 INFO mapreduce.Job: Counters: 38\
	File System Counters\
		FILE: Number of bytes read=0\
		FILE: Number of bytes written=172967\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
		HDFS: Number of bytes read=360\
		HDFS: Number of bytes written=5292105197\
		HDFS: Number of read operations=12\
		HDFS: Number of large read operations=0\
		HDFS: Number of write operations=4\
		S3: Number of bytes read=5292105197\
		S3: Number of bytes written=0\
		S3: Number of read operations=0\
		S3: Number of large read operations=0\
		S3: Number of write operations=0\
	Job Counters \
		Launched map tasks=1\
		Other local map tasks=1\
		Total time spent by all maps in occupied slots (ms)=1835808\
		Total time spent by all reduces in occupied slots (ms)=0\
		Total time spent by all map tasks (ms)=57369\
		Total vcore-milliseconds taken by all map tasks=57369\
		Total megabyte-milliseconds taken by all map tasks=58745856\
	Map-Reduce Framework\
		Map input records=1\
		Map output records=0\
		Input split bytes=136\
		Spilled Records=0\
		Failed Shuffles=0\
		Merged Map outputs=0\
		GC time elapsed (ms)=290\
		CPU time spent (ms)=59630\
		Physical memory (bytes) snapshot=982380544\
		Virtual memory (bytes) snapshot=4647981056\
		Total committed heap usage (bytes)=768081920\
	File Input Format Counters \
		Bytes Read=224\
	File Output Format Counters \
		Bytes Written=0\
	DistCp Counters\
		Bytes Copied=5292105197\
		Bytes Expected=5292105197\
		Files Copied=1\
[hadoop@ip-172-31-85-183 ~]$ hadoop fs -ls\
Found 1 items\
-rw-r--r--   1 hadoop hadoop 5292105197 2022-11-13 20:33 eng_1M_1gram\
[hadoop@ip-172-31-85-183 ~]$ pyspark\
Python 2.7.16 (default, Oct 14 2019, 21:26:56) \
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2\
Type "help", "copyright", "credits" or "license" for more information.\
Setting default log level to "WARN".\
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\
22/11/13 20:33:49 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\
22/11/13 20:33:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\
Welcome to\
      ____              __\
     / __/__  ___ _____/ /__\
    _\\ \\/ _ \\/ _ `/ __/  '_/\
   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.4\
      /_/\
\
Using Python version 2.7.16 (default, Oct 14 2019 21:26:56)\
SparkSession available as 'spark'.\
>>> df = spark.read.csv('/user/hadoop/eng_1M_1gram')\
>>> df.show(10)                                                                 \
+---------+----+---------+-----+-----+\
|      _c0| _c1|      _c2|  _c3|  _c4|\
+---------+----+---------+-----+-----+\
|    token|year|frequency|pages|books|\
|inGermany|1927|        2|    2|    2|\
|inGermany|1929|        1|    1|    1|\
|inGermany|1930|        1|    1|    1|\
|inGermany|1933|        1|    1|    1|\
|inGermany|1934|        1|    1|    1|\
|inGermany|1935|        1|    1|    1|\
|inGermany|1938|        5|    5|    5|\
|inGermany|1939|        1|    1|    1|\
|inGermany|1940|        1|    1|    1|\
+---------+----+---------+-----+-----+\
only showing top 10 rows\
\
>>> df = spark.read.csv('/user/hadoop/eng_1M_1gram', header = True)\
>>> df.show(10)\
+---------+----+---------+-----+-----+\
|    token|year|frequency|pages|books|\
+---------+----+---------+-----+-----+\
|inGermany|1927|        2|    2|    2|\
|inGermany|1929|        1|    1|    1|\
|inGermany|1930|        1|    1|    1|\
|inGermany|1933|        1|    1|    1|\
|inGermany|1934|        1|    1|    1|\
|inGermany|1935|        1|    1|    1|\
|inGermany|1938|        5|    5|    5|\
|inGermany|1939|        1|    1|    1|\
|inGermany|1940|        1|    1|    1|\
|inGermany|1942|        2|    2|    2|\
+---------+----+---------+-----+-----+\
only showing top 10 rows\
\
>>> df.count()\
261823225                                                                       \
>>> df.printSchema()\
root\
 |-- token: string (nullable = true)\
 |-- year: string (nullable = true)\
 |-- frequency: string (nullable = true)\
 |-- pages: string (nullable = true)\
 |-- books: string (nullable = true)\
\
>>> df = df.withColumn('year', df['year'].cast('int'))\
>>> df = df.withColumn('frequency', df['frequency'].cast('int'))\
>>> df = df.withColumn('pages', df['pages'].cast('int'))\
>>> df = df.withColumn('books', df['books'].cast('int'))\
>>> df.printSchema()\
root\
 |-- token: string (nullable = true)\
 |-- year: integer (nullable = true)\
 |-- frequency: integer (nullable = true)\
 |-- pages: integer (nullable = true)\
 |-- books: integer (nullable = true)\
\
>>> df.createOrReplaceTempView("data_ngram")\
>>> spark.sql('SELECT * FROM data_ngram WHERE token = "data"').show(10)\
22/11/13 20:40:56 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\
+-----+----+---------+-----+-----+                                              \
|token|year|frequency|pages|books|\
+-----+----+---------+-----+-----+\
| data|1584|       16|   14|    1|\
| data|1614|        3|    2|    1|\
| data|1627|        1|    1|    1|\
| data|1631|       22|   18|    1|\
| data|1637|        1|    1|    1|\
| data|1638|        2|    2|    1|\
| data|1640|        1|    1|    1|\
| data|1642|        1|    1|    1|\
| data|1644|        4|    4|    1|\
| data|1647|        1|    1|    1|\
+-----+----+---------+-----+-----+\
only showing top 10 rows\
\
>>> df = spark.sql('SELECT * FROM data_ngram WHERE token = "data"')\
>>> df.count()\
316                                                                             \
>>> df.printSchema()\
root\
 |-- token: string (nullable = true)\
 |-- year: integer (nullable = true)\
 |-- frequency: integer (nullable = true)\
 |-- pages: integer (nullable = true)\
 |-- books: integer (nullable = true)\
\
>>> df.write.csv('/user/hadoop/data_ngram', header = True)\
>>> quit()\
[hadoop@ip-172-31-85-183 ~]$ hadoop fs -ls\
Found 3 items\
drwxr-xr-x   - hadoop hadoop          0 2022-11-13 20:49 .sparkStaging\
drwxr-xr-x   - hadoop hadoop          0 2022-11-13 20:49 data_ngram\
-rw-r--r--   1 hadoop hadoop 5292105197 2022-11-13 20:33 eng_1M_1gram\
[hadoop@ip-172-31-85-183 ~]$ hadoop fs -ls /user/hadoop/data_ngram\
Found 3 items\
-rw-r--r--   1 hadoop hadoop          0 2022-11-13 20:49 /user/hadoop/data_ngram/_SUCCESS\
-rw-r--r--   1 hadoop hadoop          0 2022-11-13 20:48 /user/hadoop/data_ngram/part-00000-cf51506b-436d-48c8-88e8-30bcbe4bc859-c000.csv\
-rw-r--r--   1 hadoop hadoop       7305 2022-11-13 20:49 /user/hadoop/data_ngram/part-00023-cf51506b-436d-48c8-88e8-30bcbe4bc859-c000.csv\
[hadoop@ip-172-31-85-183 ~]$ hadoop fs -getmerge /user/hadoop/data_ngram /home/hadoop/final_ngram.csv\
[hadoop@ip-172-31-85-183 ~]$ sudo aws s3 cp /home/hadoop/final_ngram.csv s3://scoles-bstn-bucket/final_ngram.csv\
upload: ./final_ngram.csv to s3://scoles-bstn-bucket/final_ngram.csv}