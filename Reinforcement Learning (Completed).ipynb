{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W03-5JMk8oMW"
   },
   "source": [
    "# Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99A9L0ga8oMX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will look at reinforcement learning.\n",
    "\n",
    "To recap, supervised learning attempts to understand some relationship between a\n",
    "set of features, $X$, and a labeled target, $Y$, and unsupervised learning\n",
    "attempts to understand some relationship between a set of data points described\n",
    "only by the features $X$.\n",
    "\n",
    "Unlike those techniques, reinforcement learning tries to understand the\n",
    "relationship between taking certain actions, and the consequences of those\n",
    "actions. The simplest example for employment of reinforcement learning is within\n",
    "games like chess. Every action has a potential consequence, but a set of actions\n",
    "can only have good consequences further in time, so how do we train a system to\n",
    "study the relationship between actions and potential consequences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration-exploitation strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXuiGQHw8oNB"
   },
   "source": [
    "In reinforcement learning, the choice of an action is usually driven by two\n",
    "distinct goals: either to explore the environment and potential rewards to our\n",
    "actions, or to exploit what we learned so far. These could be two well-separated\n",
    "phases in the algorithm or somewhat interleaved steps.\n",
    "\n",
    "1. In the _exploration_ steps, our goal is not to maximize profits in the short\n",
    "   term, but instead to discover the strategy which will maximize profits in the\n",
    "   long term. In other words, we want to establish a relationship between our\n",
    "   **actions** and their associated **reward**.\n",
    "\n",
    "2. _The exploitation_ steps are where we apply the best method found in the\n",
    "   exploration phase, hopefully maximizing long term profits. We pick the\n",
    "   **action** which produces the highest **reward**, and we perform this action\n",
    "   as much as we can.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, every trial in the exploration phase is potentially costing us money\n",
    "in a real-world scenario. So, how do we discover which is the most profitable\n",
    "and how do we do so with the least cost. The first such example we look at is\n",
    "known as the multi-arm bandit problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-xwmOm88oND"
   },
   "source": [
    "<img src = \"https://drive.google.com/uc?export=view&id=1Iin2VKiGHU9sSWcp1w7WMjTrSP_4HdFe\" width = 400>\n",
    "<center><i>(Image Source: <a>https://upload.wikimedia.org/wikipedia/commons/3/34/Slot_machines_at_Wookey_Hole_Caves.JPG</a>)</i></center>\n",
    "\n",
    "The name is inspired by casino slot machines, each of which has an arm we pull\n",
    "to play (action), each machine has a different payout (different reward), and\n",
    "our job as gamblers is to discover which machine leads to the highest profits\n",
    "(while spending as little money as possible).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHebcmJs8oND"
   },
   "source": [
    "From a theoretical perspective, a multi-arm bandit is made up of $k$ machines\n",
    "(or options). We can describe their mean payouts with the array:\n",
    "\n",
    "$$(\\mu_1,\\mu_2,\\cdots ,\\mu_k)$$\n",
    "\n",
    "We don't know these means, we won't even assume we have a prior belief over\n",
    "them. Our goal is to use exploration to find the machine with the highest mean\n",
    "and then exploit this knowledge for our gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMBm4lNd8oNE"
   },
   "source": [
    "Generally, any reinforcement learning algorithm is composed of an **agent**\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=190ZAOG_E8xbF_Vi_IVmGN7NXgo_6jKQ7\" width = 100>\n",
    "\n",
    "<center><i>(Image Source: <a>https://storage.needpix.com/rsynced_images/agent-1626306_1280.png</a>)</i></center>\n",
    "\n",
    "which tries to learn a set of **actions** and navigate between **states** by\n",
    "studying the **rewards** different actions (or sequences of actions) produce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon-Greedy$\n",
    "\n",
    "As a first strategy we will employ what is known as $\\epsilon-Greedy$. This\n",
    "strategy doesn't have separate exploration/exploitation phase. Instead it\n",
    "alternates between the two. The algorithm runs trials one after another, after\n",
    "each trial it makes an estimate of the outcome means\n",
    "$(\\mu_1',\\mu_2',\\cdots ,\\mu_k')$.\n",
    "\n",
    "For each new trial, the algorithm will either execute an exploration or\n",
    "exploitation step:\n",
    "\n",
    "- In an exploration trial we select one of the $k$ options (**actions**)\n",
    "  uniformly at random, if we selected option $i$ we pull the arm on machine $i$,\n",
    "  see its recent payout (**reward**), and then we update our estimate of $\\mu_i$\n",
    "  by updating $\\mu_i'$.\n",
    "\n",
    "- In an exploitation trial, we also select a machine and pull its arm, but we\n",
    "  don't select at random. Rather we select the machine whose current estimated\n",
    "  $\\mu$ is the highest.\n",
    "\n",
    "How do we decide which method to use at each trial? We use the $\\epsilon$\n",
    "parameter which is between 0 and 1. We run an exploration trial with probability\n",
    "$\\epsilon$, otherwise (with probability $1-\\epsilon$) we execute an exploitation\n",
    "trial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's run through an example. Imagine we had the following machines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Machine Name</th>\n",
       "      <th>True Means</th>\n",
       "      <th>Number of Runs</th>\n",
       "      <th>Total Payout</th>\n",
       "      <th>Estimated Means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine 0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine 1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine 2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine 4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Machine Name  True Means  Number of Runs  Total Payout  Estimated Means\n",
       "0    Machine 0         0.2             0.0           0.0              0.0\n",
       "1    Machine 1         0.3             0.0           0.0              0.0\n",
       "2    Machine 2         0.5             0.0           0.0              0.0\n",
       "3    Machine 3         0.2             0.0           0.0              0.0\n",
       "4    Machine 4         0.9             0.0           0.0              0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_means = np.array([0.2, 0.3, 0.5, 0.2, 0.9])\n",
    "machines = pd.DataFrame(\n",
    "    {\n",
    "        \"Machine Name\": [\"Machine \" + str(i) for i in range(5)],\n",
    "        \"True Means\": machine_means,\n",
    "        \"Number of Runs\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "        \"Total Payout\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "        \"Estimated Means\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    }\n",
    ")\n",
    "machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each machine has a different payout. We know the real values here since we\n",
    "created these machines, but we will pretend our agent doesn't know them. We will\n",
    "now run an $\\epsilon$-greedy algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random coin flip\n",
    "np.random.binomial(1, 0.5, 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon, machines_df):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    epsilon: float between 0 and 1, fraction of steps for exploration\n",
    "    machines_df: bookkeeping dataframe of machines and previous games\n",
    "\n",
    "    Output: updated bookkeeping dataframe after one action\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine if this is an explore/exploit trial - biased coin flip\n",
    "    epsilon_coin = np.random.binomial(1, epsilon, 1)[0]\n",
    "\n",
    "    # Explore trial\n",
    "    if (epsilon_coin == 1) or (np.sum(machines_df[\"Estimated Means\"]) == 0.0):\n",
    "\n",
    "        # choose a machine at random\n",
    "        choice = np.random.randint(machines_df.shape[0])\n",
    "\n",
    "    # Exploit trial\n",
    "    else:\n",
    "\n",
    "        # choose the best machine\n",
    "        choice = np.argmax(machines_df[\"Estimated Means\"])\n",
    "\n",
    "    # Pull the machine of choice and update our payouts and estimated means\n",
    "    trial_result = np.random.binomial(1, machines_df.loc[choice, \"True Means\"], 1)[0]\n",
    "\n",
    "    # same with a deterministic reward if you add a Reward column\n",
    "    # trial_result = machines_df.loc[choice, \"Reward\"]\n",
    "\n",
    "    # update the bookkeeping\n",
    "    machines_df.loc[choice, \"Number of Runs\"] += 1\n",
    "    machines_df.loc[choice, \"Total Payout\"] += trial_result\n",
    "    machines_df.loc[choice, \"Estimated Means\"] = (\n",
    "        machines_df.loc[choice, \"Total Payout\"] / machines_df.loc[choice, \"Number of Runs\"]\n",
    "    )\n",
    "\n",
    "    # return the updated estimated means\n",
    "    return machines_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to run this strategy with five machines for 10,000 trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial 9999\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Machine Name</th>\n",
       "      <th>True Means</th>\n",
       "      <th>Number of Runs</th>\n",
       "      <th>Total Payout</th>\n",
       "      <th>Estimated Means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine 0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>0.205245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine 1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>0.299398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine 2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.501961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.205794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine 4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>11981.0</td>\n",
       "      <td>10755.0</td>\n",
       "      <td>0.897671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Machine Name  True Means  Number of Runs  Total Payout  Estimated Means\n",
       "0    Machine 0         0.2          1983.0         407.0         0.205245\n",
       "1    Machine 1         0.3          1994.0         597.0         0.299398\n",
       "2    Machine 2         0.5          2040.0        1024.0         0.501961\n",
       "3    Machine 3         0.2          2002.0         412.0         0.205794\n",
       "4    Machine 4         0.9         11981.0       10755.0         0.897671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trials = 10000\n",
    "epsilon = 0.1  # 10% of the time, we explore\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(f\"Running Trial {i}\", end=\"\\r\")\n",
    "    machines = epsilon_greedy(epsilon, machines)\n",
    "#     if i % 1000 == 0:\n",
    "#         print(machines)\n",
    "\n",
    "machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can consider $\\epsilon$ as a hyperparameter: some amount of exploration is\n",
    "good but too much will unnecessarily pick actions that we already know don't\n",
    "work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13195.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines[\"Total Payout\"].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0KdboRG8oNK"
   },
   "source": [
    "First, notice that the estimated means are pretty close to the true means, so\n",
    "the strategy has learned what the machine payouts are. But is the strategy a\n",
    "profitable one?\n",
    "\n",
    "The measure of profit in the multi-arm bandit setting is called _regret\n",
    "minimization_. Regret is the difference between the best we could have had (in\n",
    "expectation) and what we actually got. What is the best possible outcome? In\n",
    "this particular example the best strategy is to always pick the machine with the\n",
    "highest mean ($\\mu_{opt}$). So after $T$ rounds the optimal strategy would pay\n",
    "us $T \\cdot \\mu_{opt}$ in expectation, instead by picking machine $i$ in round\n",
    "$j$ we got payment $p_j^i$. So after $T$ rounds our overall regret is:\n",
    "\n",
    "$$T \\cdot \\mu_{opt} - \\sum_{j=1}^T p_j^i$$\n",
    "\n",
    "We can calculate this with the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1ErNITG8oNK"
   },
   "outputs": [],
   "source": [
    "def regret(machine_df):\n",
    "    optimal_machine = np.max(machine_df[\"True Means\"])\n",
    "    trial_number = np.sum(machine_df[\"Number of Runs\"])\n",
    "\n",
    "    optimal_payout = trial_number * optimal_machine\n",
    "    actual_payout = np.sum(machine_df[\"Total Payout\"])\n",
    "    return optimal_payout - actual_payout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_IevPym8oNN"
   },
   "source": [
    "And get the overall regret of our strategy with the following call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCoL4HVc8oNP",
    "outputId": "bdfb108f-666b-4ef2-9581-136b74dcc9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Regret: 4805.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Regret: {regret(machines)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkDTCBHZ8oNU"
   },
   "source": [
    "Is this regret good or bad? It's hard to say. To use regret we should compare it\n",
    "against other strategies and try to find the one with the overall minimal\n",
    "regret.\n",
    "\n",
    "One problem with the regret measure is it require that we know what the overall\n",
    "optimal machine is. Obviously if we knew this we would not require an\n",
    "exploration phase. Instead when calculating regret we can make an estimate of\n",
    "the optimal, using the knowledge we've accumulated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Exercise 1\n",
    "\n",
    "1. Implement regret in each round of the $\\epsilon-Greedy$ strategy. That is,\n",
    "augment our the code in the function above so that we record the regret of the\n",
    "strategy after every move. Plot the round by round regret of this strategy.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will record the (cumulative) regret over the game and then plot the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial 9999\r"
     ]
    }
   ],
   "source": [
    "# reset the bookkeeping dataframe\n",
    "machine_means = np.array([0.2, 0.3, 0.5, 0.2, 0.9])\n",
    "machines = pd.DataFrame(\n",
    "    {\n",
    "        \"Machine Name\": [\"Machine \" + str(i) for i in range(5)],\n",
    "        \"True Means\": machine_means,\n",
    "        \"Number of Runs\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "        \"Total Payout\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "        \"Estimated Means\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# trial runs\n",
    "num_trials = 10000\n",
    "epsilon = 0.1  # 10% of the time, we explore\n",
    "\n",
    "regrets = []\n",
    "for i in range(num_trials):\n",
    "    print(f\"Running Trial {i}\", end=\"\\r\")\n",
    "    machines = epsilon_greedy(epsilon, machines)\n",
    "\n",
    "    # calculate regret\n",
    "    current_regret = regret(machines)\n",
    "\n",
    "    # save regret\n",
    "    regrets.append(current_regret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_trials), regrets)\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Cumulative regret over 10000 trials\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Machine Name</th>\n",
       "      <th>True Means</th>\n",
       "      <th>Number of Runs</th>\n",
       "      <th>Total Payout</th>\n",
       "      <th>Estimated Means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine 0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>188.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine 1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>209.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.306220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine 2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>205.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.144385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine 4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>9211.0</td>\n",
       "      <td>8286.0</td>\n",
       "      <td>0.899577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Machine Name  True Means  Number of Runs  Total Payout  Estimated Means\n",
       "0    Machine 0         0.2           188.0          40.0         0.212766\n",
       "1    Machine 1         0.3           209.0          64.0         0.306220\n",
       "2    Machine 2         0.5           205.0         107.0         0.521951\n",
       "3    Machine 3         0.2           187.0          27.0         0.144385\n",
       "4    Machine 4         0.9          9211.0        8286.0         0.899577"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how initially the regret grew much more steeply but as we learned the\n",
    "optimal strategy to choose the best machine (in the first few hundred games) the\n",
    "growth rate stabilized. Note that the regret keeps growing because we still play\n",
    "non-optimally in the exploration phase 10% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muRrlqI38oNU"
   },
   "source": [
    "### $\\epsilon-Decay$ Strategy\n",
    "\n",
    "As mentioned above, we usually have multple strategies that we compare using\n",
    "regret.\n",
    "\n",
    "$\\epsilon-Decay$ is pretty much the same as $\\epsilon-Greedy$, except we start\n",
    "with a higher $\\epsilon$ and slowly decrease it over time.\n",
    "\n",
    "We usually try various decay techniques, such as\n",
    "$\\epsilon_{new} = \\epsilon_{old} - c$ or\n",
    "$\\epsilon_{new} = \\frac{\\epsilon_{old}}{c}$. We then compare the regret over\n",
    "time of new strategies (along with the original $\\epsilon-Greedy$) in order to\n",
    "select the best strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Rewards\n",
    "\n",
    "Looking at the information we've collected while running our epsilon greedy\n",
    "algorithm, we can look at two columns: The machine, and its associated payout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Machine Name</th>\n",
       "      <th>Estimated Means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine 0</td>\n",
       "      <td>0.175926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine 1</td>\n",
       "      <td>0.335052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine 2</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine 3</td>\n",
       "      <td>0.199005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine 4</td>\n",
       "      <td>0.898117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Machine Name  Estimated Means\n",
       "0    Machine 0         0.175926\n",
       "1    Machine 1         0.335052\n",
       "2    Machine 2         0.544554\n",
       "3    Machine 3         0.199005\n",
       "4    Machine 4         0.898117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines[[\"Machine Name\", \"Estimated Means\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can re-frame these information pieces as **actions** and **rewards**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine 0</td>\n",
       "      <td>0.188172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine 1</td>\n",
       "      <td>0.287938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine 2</td>\n",
       "      <td>0.496503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine 3</td>\n",
       "      <td>0.169014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine 4</td>\n",
       "      <td>0.898764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Action    Reward\n",
       "0  Machine 0  0.188172\n",
       "1  Machine 1  0.287938\n",
       "2  Machine 2  0.496503\n",
       "3  Machine 3  0.169014\n",
       "4  Machine 4  0.898764"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = machines[[\"Machine Name\", \"Estimated Means\"]].copy()\n",
    "new_df.columns = [\"Action\", \"Reward\"]\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in terms of information, we really have a $5 \\times 1$ information vector\n",
    "which says that whenever we take action $i$ we get reward $reward[i]$, then we\n",
    "are thrown back into the same **state** to choose which machine to pull again.\n",
    "\n",
    "In this setup, we of course will always pull machine 4... but what if we had a\n",
    "slightly unusual set-up where we can't pull the same machine twice in a row? Now\n",
    "we might pull machine 4's arm, and get our reward, but we cannot do it again. We\n",
    "are in a new **state** and our available choices are different than they were\n",
    "one turn ago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clLvE3VM8oNX"
   },
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "Multi-armed bandits are a special case of Markov Decision Processes (MDPs). In\n",
    "an MDP the world is divided into a set of states $S$, each state $s \\in S$ has a\n",
    "set of actions $A_s$, an action $a \\in A_s$ transitions us from state $s$ to\n",
    "some other state $s'$ and the action has some (possibly random) reward $R_a$.\n",
    "\n",
    "The multi-armed bandit is a MDP with one state, the actions, lever pulls, lead\n",
    "back to the same state. In general MDPs can be far more complex:\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1G894t7UGZdNnDIzwQDUrRz7s7-2V7gAo\"  width = 500>\n",
    "<center><i>(Image Source: <a>https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png</a>)</i></center>\n",
    "\n",
    "Our goal with MDPs is the same as our goal with multi arm bandit. We need to\n",
    "explore the system and discover the structure and payments of the systems and\n",
    "exploit this knowledge to maximize our payouts. Critically we do not know\n",
    "anything about the structure of the MDP a priori, we must discover this through\n",
    "exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZz4mxok8oNY"
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "Since the 1980s the standard way for dealing with MDPs has been the Q-Learning\n",
    "algorithm. Q-Learning, shorthand for Quality Function Learning, is a general\n",
    "methodology for learning the payment structure of the MDP. It learns to\n",
    "associate with every state $s$, and an action (accessible from the state) $a$, a\n",
    "quality score $Q(s,a)$. $Q(s,a)$ is meant to approximate the quality of the\n",
    "outcome taking action $a$ at state $s$ will eventually lead to.\n",
    "\n",
    "If we knew the structure of the MDP we could easily take the optimal path\n",
    "through the states minimizing our cost (equivalently maximizing our reward). But\n",
    "since we don't know what structure looks like, connections or payments, we need\n",
    "to explore and learn which paths lead to good outcomes, that is we need to learn\n",
    "the $Q$ function via exploration. With a good Q-Function we can exploit our\n",
    "knowledge and maximize our profits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of the Q-Learning algorithm is a Q-table; A table which stores\n",
    "information about the world as experienced by an agent (in the Multi-armed\n",
    "Bandit, we had an automatic agent populating a $5\\times 1$ Q-table). Let's\n",
    "imagine we have a world of a $3\\times 3$ tiles, and an agent which has several\n",
    "actions available: move up, move down, move left, and move right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>            \n",
    "        </th>\n",
    "        <th>\n",
    "            Up\n",
    "        </th>\n",
    "        <th>\n",
    "            Down\n",
    "        </th>\n",
    "        <th>\n",
    "            Left\n",
    "        </th>\n",
    "        <th>\n",
    "            Right\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Upper Left\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Upper Middle\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Upper Right\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Middle Left\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Middle Middle\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Middle Right\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Bottom Left\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Bottom Middle\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Bottom Right\n",
    "        </th>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>\n",
    "        <td>  \n",
    "            0.0\n",
    "        </td>        \n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can represent as a matrix:\n",
    "\n",
    "$$\n",
    "Q\\_table = \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "                            0.0 & 0.0 & 0.0 & 0.0 \\\\\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it starts out as all zeros because our agent knows nothing about the\n",
    "world. Now let's say our agent makes an action, receives some reward, and now\n",
    "has to update its understanding of the world, how would it update its\n",
    "understanding?\n",
    "\n",
    "The agent updates its understanding based on the following rule called Bellman's\n",
    "equation:\n",
    "\n",
    "$$Q_{new}(s,a) = (1-\\alpha) \\cdot Q_{old}(s,a) + \\alpha \\cdot (r + \\gamma  (max_{a'}Q(s',a'))$$\n",
    "\n",
    "Where: <br>\n",
    "\n",
    "- $\\alpha$ is the learning rate that controls how fast the Q-values are changing\n",
    "  (the smaller $\\alpha$ is, the slower the change), <br>\n",
    "- $r$ is the reward received for performing action $a$ in state $s$, <br>\n",
    "- $\\gamma$ is a discount parameter which determines how much it discounts future\n",
    "  rewards when making the update,<br>\n",
    "- $max_{a'}Q(s',a')$ is the maximum $Q$ value from the new state we move into.\n",
    "  In essence, the agent looks ahead and asks \"if I take the action to take me\n",
    "  into state $s'$, what is the highest reward I can get there?\"\n",
    "\n",
    "Notice that if $\\gamma = 0$, then the rule becomes\n",
    "$$Q_{new}(s,a) = (1-\\alpha)\\cdot Q_{old}(s,a) + \\alpha\\cdot r$$ which means we\n",
    "only take into account the immediate reward we just got.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a `Turtle` module that has been expanded to jupyter notebooks, let's\n",
    "employ it to illustrate Q-learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_tkinter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mturtleScript\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# A few parameters to intialize\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tile_number \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[0;32m~/src/work/brainstation/datascience-bootcamp/Unit_4 - Big Data/3-Adanced Big Data and ML Methods/turtleScript.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mturtle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/turtle.py:107\u001b[0m\n\u001b[1;32m    103\u001b[0m _ver \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mturtle 1.1b- - for Python 3.1   -  4. 5. 2009\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[39m# print(_ver)\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mTK\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tkinter/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m_tkinter\u001b[39;00m \u001b[39m# If this fails your Python may not be configured for Tk\u001b[39;00m\n\u001b[1;32m     38\u001b[0m TclError \u001b[39m=\u001b[39m _tkinter\u001b[39m.\u001b[39mTclError\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_tkinter'"
     ]
    }
   ],
   "source": [
    "from turtleScript import *\n",
    "\n",
    "# A few parameters to intialize\n",
    "\n",
    "tile_number = 3\n",
    "t = QTurtle(TILE_NUMBER=tile_number)\n",
    "display(t)\n",
    "t.reset_world()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our turtle here is the agent, and let's pretend there is food on the bottom\n",
    "right square. We want to teach our turtle that it should move towards there. How\n",
    "will that work?\n",
    "\n",
    "Let's begin with an empty Q-table, it will have $tile\\_number^2 \\times 4$\n",
    "entries. 4 entries of up,down, left, and right for each square on the board.\n",
    "Let's also create a representation of the world, with some big reward in the\n",
    "bottom right square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.],\n",
       "       [  0.,   0., 100.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "World = np.zeros((tile_number, tile_number))\n",
    "World[-1, -1] = 100\n",
    "display(World)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Up  Down  Left  Right\n",
       "Top Left       0.0   0.0   0.0    0.0\n",
       "Top Middle     0.0   0.0   0.0    0.0\n",
       "Top Right      0.0   0.0   0.0    0.0\n",
       "Middle Left    0.0   0.0   0.0    0.0\n",
       "Middle Middle  0.0   0.0   0.0    0.0\n",
       "Middle Right   0.0   0.0   0.0    0.0\n",
       "Bottom Left    0.0   0.0   0.0    0.0\n",
       "Bottom Middle  0.0   0.0   0.0    0.0\n",
       "Bottom Right   0.0   0.0   0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q_table = pd.DataFrame(\n",
    "    columns=[\"Up\", \"Down\", \"Left\", \"Right\"],\n",
    "    index=[\n",
    "        \"Top Left\",\n",
    "        \"Top Middle\",\n",
    "        \"Top Right\",\n",
    "        \"Middle Left\",\n",
    "        \"Middle Middle\",\n",
    "        \"Middle Right\",\n",
    "        \"Bottom Left\",\n",
    "        \"Bottom Middle\",\n",
    "        \"Bottom Right\",\n",
    "    ],\n",
    "    data=np.zeros((tile_number * tile_number, 4)),\n",
    ")\n",
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our turtle move one square right, and one square down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state was: 4\n"
     ]
    }
   ],
   "source": [
    "state = t.get_Q_state()\n",
    "print(f\"state was: {state}\")\n",
    "t.move_right()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to update here, we got no reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state was: 5\n"
     ]
    }
   ],
   "source": [
    "state = t.get_Q_state()\n",
    "print(f\"state was: {state}\")\n",
    "t.move_down()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update the Q-table, we want to update the value of the middle\n",
    "rightmost square so we know the best action from that place is going straight\n",
    "down.\n",
    "\n",
    "We've yet to set our Q-learning parameters so let's do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # learning rate parameter, how fast are we adjusting the Q-values\n",
    "gamma = 0.9  # discount parameter, how much we take the future rewards account\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now apply the rule\n",
    "$Q_{new}(s,a) = (1-\\alpha) \\cdot Q_{old}(s,a) + \\alpha \\cdot (r + \\gamma  (max_{a'}Q(s',a'))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New state is: 8\n"
     ]
    }
   ],
   "source": [
    "new_state = t.get_Q_state()\n",
    "print(f\" New state is: {new_state}\")\n",
    "reward = World[-1, -1]  # 100\n",
    "\n",
    "Q_table.iloc[state].loc[\"Down\"] = ((1 - alpha) * Q_table.iloc[state].loc[\"Down\"]) + (\n",
    "    alpha * (reward + (gamma * np.max(Q_table.iloc[new_state])))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Up  Down  Left  Right\n",
       "Top Left       0.0   0.0   0.0    0.0\n",
       "Top Middle     0.0   0.0   0.0    0.0\n",
       "Top Right      0.0   0.0   0.0    0.0\n",
       "Middle Left    0.0   0.0   0.0    0.0\n",
       "Middle Middle  0.0   0.0   0.0    0.0\n",
       "Middle Right   0.0  10.0   0.0    0.0\n",
       "Bottom Left    0.0   0.0   0.0    0.0\n",
       "Bottom Middle  0.0   0.0   0.0    0.0\n",
       "Bottom Right   0.0   0.0   0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the quality of going down from state \"Middle Right\" has changed from\n",
    "0 to 10. Now when getting to state 5 (the rightmost middle row tile) the agent\n",
    "will know its best bet is to go down.\n",
    "\n",
    "But wait, will it just randomly wander until it reaches only adjacent tiles?\n",
    "\n",
    "Luckily no. Let's see what happens when our agent begins in the top right corner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.drop_in_cell(0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Up  Down  Left  Right\n",
       "Top Left       0.0   0.0   0.0    0.0\n",
       "Top Middle     0.0   0.0   0.0    0.0\n",
       "Top Right      0.0   0.0   0.0    0.0\n",
       "Middle Left    0.0   0.0   0.0    0.0\n",
       "Middle Middle  0.0   0.0   0.0    0.0\n",
       "Middle Right   0.0  10.0   0.0    0.0\n",
       "Bottom Left    0.0   0.0   0.0    0.0\n",
       "Bottom Middle  0.0   0.0   0.0    0.0\n",
       "Bottom Right   0.0   0.0   0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the Q-table\n",
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state was: 2\n"
     ]
    }
   ],
   "source": [
    "state = t.get_Q_state()\n",
    "print(f\"state was: {state}\")\n",
    "t.move_down()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we update our Q-table again. Notice our agent gets no reward from moving\n",
    "down, BUT its new state says there is a reward for being in this new square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state is: 5\n",
      "state was: 2\n"
     ]
    }
   ],
   "source": [
    "new_state = t.get_Q_state()\n",
    "print(f\"New state is: {new_state}\")\n",
    "print(f\"state was: {state}\")\n",
    "turtle_row, turtle_column = t.get_cell()\n",
    "reward = World[turtle_row, turtle_column]\n",
    "\n",
    "Q_table.iloc[state].loc[\"Down\"] = ((1 - alpha) * Q_table.iloc[state].loc[\"Down\"]) + (\n",
    "    alpha * (reward + (gamma * np.max(Q_table.iloc[new_state])))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down every part of this new update. First, the old value\n",
    "$(1-\\alpha) \\cdot Q_{old}(s,a)$ is pretty clear, it's the previous value of that\n",
    "state, action pair.\n",
    "\n",
    "Next, the reward portion $r$. What is the value of that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward =  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"reward = \", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no reward associated with going to the middle right square, there is\n",
    "no food there.\n",
    "\n",
    "However, what about the third piece, the discounted reward\n",
    "$\\gamma  (max_{a'}Q(s',a')$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state actions' qualities:\n",
      " Up        0.0\n",
      "Down     10.0\n",
      "Left      0.0\n",
      "Right     0.0\n",
      "Name: Middle Right, dtype: float64\n",
      "\n",
      "Best new state action quality:\n",
      " 10.0\n",
      "\n",
      "Best new state action quality discounted:\n",
      " 9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"New state actions' qualities:\\n\", Q_table.iloc[new_state], end=\"\\n\\n\")\n",
    "print(\"Best new state action quality:\\n\", np.max(Q_table.iloc[new_state]), end=\"\\n\\n\")\n",
    "print(\"Best new state action quality discounted:\\n\", gamma * np.max(Q_table.iloc[new_state]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our new value for $$\\alpha \\cdot (r + \\gamma  (max_{a'}Q(s',a'))$$ is\n",
    "$$0.1 \\cdot (0.0 + 0.9 \\cdot 10.0) = 0.9$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Up  Down  Left  Right\n",
       "Top Left       0.0   0.0   0.0    0.0\n",
       "Top Middle     0.0   0.0   0.0    0.0\n",
       "Top Right      0.0   0.9   0.0    0.0\n",
       "Middle Left    0.0   0.0   0.0    0.0\n",
       "Middle Middle  0.0   0.0   0.0    0.0\n",
       "Middle Right   0.0  10.0   0.0    0.0\n",
       "Bottom Left    0.0   0.0   0.0    0.0\n",
       "Bottom Middle  0.0   0.0   0.0    0.0\n",
       "Bottom Right   0.0   0.0   0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how for state \"Top Right\", there is now a small value associated with\n",
    "going down. Why? because the quality of the cell right below it is very high\n",
    "(since one cell further down is the reward, food).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast forward the process...\n",
    "\n",
    "Let's move this process further along. Let's land the turtle at random\n",
    "positions, and use an epsilon greedy approach for learning about the world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_turtle(epsilon, turtle, Q_table, World):\n",
    "\n",
    "    epsilon_coin = np.random.binomial(1, epsilon, 1)[0]\n",
    "\n",
    "    directions = [turtle.move_up, turtle.move_down, turtle.move_left, turtle.move_right]\n",
    "\n",
    "    state = turtle.get_Q_state()\n",
    "    direction = None\n",
    "    # Explore trial\n",
    "    if epsilon_coin == 1:\n",
    "\n",
    "        direction = np.random.randint(len(directions))\n",
    "\n",
    "    # Exploit trial\n",
    "    else:\n",
    "\n",
    "        direction = np.argmax(Q_table.iloc[state].values)\n",
    "\n",
    "    movement_function = directions[direction]  # this is a callable function\n",
    "    movement_function()\n",
    "\n",
    "    turtle_row, turtle_column = turtle.get_cell()\n",
    "    new_state = t.get_Q_state()\n",
    "    reward = World[turtle_row, turtle_column]\n",
    "\n",
    "    Q_table.iloc[state, direction] = ((1 - alpha) * Q_table.iloc[state, direction]) + (\n",
    "        alpha * (reward + (gamma * np.max(Q_table.iloc[new_state])))\n",
    "    )\n",
    "\n",
    "    # return the updated table\n",
    "    return Q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running 500 update steps with the grid environment and the Q-table. At\n",
    "the end of each episode (after we got the reward), we drop the turtle at a\n",
    "random start position (`exploring starts`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 99.8% of run\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "step_number = 0\n",
    "epsilon = 0.5\n",
    "loop_number = 500\n",
    "for i in range(loop_number):\n",
    "    turtle_row, turtle_column = t.get_cell()\n",
    "    if World[turtle_row, turtle_column] == 100.0:\n",
    "\n",
    "        # If the turtle found food, reset it and throw it into a random new position\n",
    "\n",
    "        random_row = np.random.randint(World.shape[0])\n",
    "        random_column = np.random.randint(World.shape[1])\n",
    "        step_number = 0\n",
    "        t.drop_in_cell(random_row, random_column)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        # If it didn't find food, keep searching and learning\n",
    "\n",
    "        step_number += 1\n",
    "        Q_table = epsilon_greedy_turtle(epsilon, t, Q_table, World)\n",
    "    print(f\"finished {i*100/loop_number}% of run\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>10.425453</td>\n",
       "      <td>7.626582</td>\n",
       "      <td>2.010414</td>\n",
       "      <td>38.943865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>18.584538</td>\n",
       "      <td>19.824016</td>\n",
       "      <td>5.752503</td>\n",
       "      <td>67.426614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>35.789917</td>\n",
       "      <td>87.227449</td>\n",
       "      <td>25.881357</td>\n",
       "      <td>35.492692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>10.443475</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>4.162011</td>\n",
       "      <td>47.004587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>20.506167</td>\n",
       "      <td>15.816151</td>\n",
       "      <td>11.498423</td>\n",
       "      <td>77.949370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>43.278584</td>\n",
       "      <td>99.854442</td>\n",
       "      <td>22.343547</td>\n",
       "      <td>62.608747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>16.378020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574152</td>\n",
       "      <td>3.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.217031</td>\n",
       "      <td>2.021887</td>\n",
       "      <td>52.170310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Up       Down       Left      Right\n",
       "Top Left       10.425453   7.626582   2.010414  38.943865\n",
       "Top Middle     18.584538  19.824016   5.752503  67.426614\n",
       "Top Right      35.789917  87.227449  25.881357  35.492692\n",
       "Middle Left    10.443475   0.000720   4.162011  47.004587\n",
       "Middle Middle  20.506167  15.816151  11.498423  77.949370\n",
       "Middle Right   43.278584  99.854442  22.343547  62.608747\n",
       "Bottom Left    16.378020   0.000000   0.574152   3.095100\n",
       "Bottom Middle   0.000000   4.217031   2.021887  52.170310\n",
       "Bottom Right    0.000000   0.000000   0.000000   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 99.0% of run\r"
     ]
    }
   ],
   "source": [
    "epsilon = 0.001  # Really not likely to explore, only exploit (i.e. only follow the best path)\n",
    "\n",
    "loop_number = 100\n",
    "for i in range(loop_number):\n",
    "    turtle_row, turtle_column = t.get_cell()\n",
    "    if World[turtle_row, turtle_column] == 100.0:\n",
    "        random_row = np.random.randint(World.shape[0])\n",
    "        random_column = np.random.randint(World.shape[1])\n",
    "        t.drop_in_cell(random_row, random_column)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "\n",
    "        Q_table = epsilon_greedy_turtle(epsilon, t, Q_table, World)\n",
    "\n",
    "    print(f\"finished {i*100/loop_number}% of run\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Exercise 2\n",
    "\n",
    "1. The way we set up our reward system, our agent might not learn the shortest\n",
    "path to the food from every point. Try dropping the turtle in cell (2,0) to see\n",
    "how it moves towards the food. How can we modify our reward system to make it\n",
    "more likely it learns the shortest path from every point?\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "We start by setting up an empty Q-table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Up  Down  Left  Right\n",
       "Top Left       0.0   0.0   0.0    0.0\n",
       "Top Middle     0.0   0.0   0.0    0.0\n",
       "Top Right      0.0   0.0   0.0    0.0\n",
       "Middle Left    0.0   0.0   0.0    0.0\n",
       "Middle Middle  0.0   0.0   0.0    0.0\n",
       "Middle Right   0.0   0.0   0.0    0.0\n",
       "Bottom Left    0.0   0.0   0.0    0.0\n",
       "Bottom Middle  0.0   0.0   0.0    0.0\n",
       "Bottom Right   0.0   0.0   0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q_table = pd.DataFrame(\n",
    "    columns=[\"Up\", \"Down\", \"Left\", \"Right\"],\n",
    "    index=[\n",
    "        \"Top Left\",\n",
    "        \"Top Middle\",\n",
    "        \"Top Right\",\n",
    "        \"Middle Left\",\n",
    "        \"Middle Middle\",\n",
    "        \"Middle Right\",\n",
    "        \"Bottom Left\",\n",
    "        \"Bottom Middle\",\n",
    "        \"Bottom Right\",\n",
    "    ],\n",
    "    data=np.zeros((tile_number * tile_number, 4)),\n",
    ")\n",
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize for a shortest path, we will give a negative reward for\n",
    "stepping on any of the non-final positions (the value -10 was chosen quite\n",
    "arbitrarily):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.],\n",
       "       [  0.,   0., 100.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10., -10., -10.],\n",
       "       [-10., -10., -10.],\n",
       "       [-10., -10., 100.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_World = np.where(World == 0, -10, World)\n",
    "new_World\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can rerun the training loop and see how the learned Q-values and paths\n",
    "changed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 99.8% of run\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "step_number = 0\n",
    "epsilon = 0.5\n",
    "loop_number = 500\n",
    "for i in range(loop_number):\n",
    "    turtle_row, turtle_column = t.get_cell()\n",
    "    if new_World[turtle_row, turtle_column] == 100.0:\n",
    "\n",
    "        # If the turtle found food, reset it and throw it into a random new position\n",
    "\n",
    "        random_row = np.random.randint(new_World.shape[0])\n",
    "        random_column = np.random.randint(new_World.shape[1])\n",
    "        step_number = 0\n",
    "        t.drop_in_cell(random_row, random_column)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        # If it didn't find food, keep searching and learning\n",
    "\n",
    "        step_number += 1\n",
    "        Q_table = epsilon_greedy_turtle(epsilon, t, Q_table, new_World)\n",
    "    print(f\"finished {i*100/loop_number}% of run\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Up</th>\n",
       "      <th>Down</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top Left</th>\n",
       "      <td>-3.903111</td>\n",
       "      <td>4.968770</td>\n",
       "      <td>-4.823637</td>\n",
       "      <td>14.447770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Middle</th>\n",
       "      <td>2.317211</td>\n",
       "      <td>48.919410</td>\n",
       "      <td>-4.818776</td>\n",
       "      <td>19.234927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top Right</th>\n",
       "      <td>18.683642</td>\n",
       "      <td>70.181829</td>\n",
       "      <td>-3.491850</td>\n",
       "      <td>12.239481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Left</th>\n",
       "      <td>-2.072329</td>\n",
       "      <td>-2.565656</td>\n",
       "      <td>4.929495</td>\n",
       "      <td>43.399325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Middle</th>\n",
       "      <td>10.267236</td>\n",
       "      <td>8.709896</td>\n",
       "      <td>14.191838</td>\n",
       "      <td>77.250168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Right</th>\n",
       "      <td>17.603635</td>\n",
       "      <td>99.695675</td>\n",
       "      <td>39.088342</td>\n",
       "      <td>48.735788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Left</th>\n",
       "      <td>-3.990412</td>\n",
       "      <td>-4.397433</td>\n",
       "      <td>-3.940399</td>\n",
       "      <td>26.956751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Middle</th>\n",
       "      <td>19.146688</td>\n",
       "      <td>11.128144</td>\n",
       "      <td>-3.160213</td>\n",
       "      <td>74.581342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom Right</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Up       Down       Left      Right\n",
       "Top Left       -3.903111   4.968770  -4.823637  14.447770\n",
       "Top Middle      2.317211  48.919410  -4.818776  19.234927\n",
       "Top Right      18.683642  70.181829  -3.491850  12.239481\n",
       "Middle Left    -2.072329  -2.565656   4.929495  43.399325\n",
       "Middle Middle  10.267236   8.709896  14.191838  77.250168\n",
       "Middle Right   17.603635  99.695675  39.088342  48.735788\n",
       "Bottom Left    -3.990412  -4.397433  -3.940399  26.956751\n",
       "Bottom Middle  19.146688  11.128144  -3.160213  74.581342\n",
       "Bottom Right    0.000000   0.000000   0.000000   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 99.0% of run\r"
     ]
    }
   ],
   "source": [
    "epsilon = 0.001  # Really not likely to explore, only exploit (i.e. only follow the best path)\n",
    "\n",
    "loop_number = 100\n",
    "for i in range(loop_number):\n",
    "    turtle_row, turtle_column = t.get_cell()\n",
    "\n",
    "    if new_World[turtle_row, turtle_column] == 100.0:\n",
    "        random_row = np.random.randint(new_World.shape[0])\n",
    "        random_column = np.random.randint(new_World.shape[1])\n",
    "        t.drop_in_cell(random_row, random_column)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "\n",
    "        Q_table = epsilon_greedy_turtle(epsilon, t, Q_table, new_World)\n",
    "\n",
    "    print(f\"finished {i*100/loop_number}% of run\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see that the turtle is taking the shortest path now, as\n",
    "expected. One way to visualize this is to show the Q-values of each action over\n",
    "the state-space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show update Q values\n",
    "plt.subplots(1, 4, figsize=(10, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(Q_table[\"Up\"].values.reshape(3, 3), vmin=Q_table.values.min(), vmax=Q_table.values.max())\n",
    "plt.title(\"Up\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(Q_table[\"Down\"].values.reshape(3, 3), vmin=Q_table.values.min(), vmax=Q_table.values.max())\n",
    "plt.title(\"Down\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(Q_table[\"Left\"].values.reshape(3, 3), vmin=Q_table.values.min(), vmax=Q_table.values.max())\n",
    "plt.title(\"Left\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(Q_table[\"Right\"].values.reshape(3, 3), vmin=Q_table.values.min(), vmax=Q_table.values.max())\n",
    "plt.title(\"Right\")\n",
    "plt.suptitle(\"Q-values per actions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lighter colors show higher Q-values so for example, from middle right the\n",
    "best choice now is going down; from middle bottom, we should go right; as\n",
    "expected, going up or left has little value overall when we look for the\n",
    "shortest path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9dEY3ko8oNc"
   },
   "source": [
    "All of the methods we've seen up to this point are examples of reinforcement\n",
    "learning. Unlike supervised learning, classification with SVMs or fitting\n",
    "regression functions, we don't have an explicit target we are trying to predict\n",
    "we don't have class values or a function we are trying to fit. It is also unlike\n",
    "unsupervised learning, we have information about which outcomes are good and\n",
    "bad, we are optimizing for something explicit.\n",
    "\n",
    "The common theme among all these methods is we have some signal that indicates\n",
    "how we are doing, we use this signal to learn about the environment we are\n",
    "working in, and try and maximize our overall score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maiXO1qL8oNd"
   },
   "source": [
    "Probably the most famous examples of reinforcement learning comes from learning\n",
    "to play games. For example consider the game of Pong. Pong can be formulated as\n",
    "a MDP. Let's say we wanted to train some model to control the left paddle. To\n",
    "this model a state is just a picture of the screen:\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1ylg7XS4BlJym0pOYITQ2HDuFZZXWq_MN\" width = 400 >\n",
    "<center><i>(Image Source: <a>https://upload.wikimedia.org/wikipedia/commons/f/f8/Pong.png</a>)</i></center>\n",
    "\n",
    "The actions are the the directions we can move our paddle (up, down or not at\n",
    "all).\n",
    "\n",
    "The objective is to maximize the score. We can't use traditional Q-Learning with\n",
    "this problem since the number of possible states is huge. It would take forever\n",
    "to converge to anything reasonable. Instead the standard technique is to now use\n",
    "a neural network:\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1C_kEnG6gryrih4hJESiFR-welgUL5tc2\" width=400>\n",
    "<center><i>(Image Source: <a>https://i.imgur.com/0Cow9XW.png</a>)</i></center>\n",
    "\n",
    "The network takes the state, represented as an image (or another appropriate\n",
    "representation), and outputs an indication of which is the next best action.\n",
    "\n",
    "This process is called Deep Q Learning, and is roughly divided into two main\n",
    "categories:\n",
    "\n",
    "#### Value-based Learning\n",
    "\n",
    "Value based learning is appropriate for cases where the set of all possible\n",
    "states is huge, but the set of all possible actions is finite and small. In\n",
    "Value based learning, the network takes as input a state $S$ and outputs the\n",
    "quality of all possible actions. If a given game has only four controls (e.g.\n",
    "Up, Down, Left, Right), then the network outputs 4 qualities (the output layer\n",
    "will have 4 neurons, but this is **NOT** a classification task): $Q(S, Up)$,\n",
    "$Q(S, Down)$, $Q(S, Left)$, $Q(S, Right)$.\n",
    "\n",
    "#### Policy-based Learning\n",
    "\n",
    "Policy based learning tackles different games or situations where both the state\n",
    "space and action space are very large. For these situations, given a possibility\n",
    "of 40 actions, the network output layer will still have 40 output neurons.\n",
    "However, it now becomes a classification problem. Formally speaking, given a\n",
    "state $S$ we know want our network to learn $\\pi(a_i|S)$, the probability of\n",
    "action $a_i$ for state S.\n",
    "\n",
    "Consider a game of chess. Instead of asking for the quality of each possible\n",
    "action, we ask the network to output the probability every action should be\n",
    "taken. Since this is rephrased as a classification task, we can make the network\n",
    "run through experts' games at first, and learn some basic strategies (policies)\n",
    "based on their games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyYAnq1G8oNd"
   },
   "source": [
    "### But Games Don't Have Instant Rewards...\n",
    "\n",
    "It's true. Consider again a game of chess. Perhaps we performed a set of\n",
    "actions: <br>\n",
    "$a_1 \\rightarrow a_2 \\rightarrow a_3 \\rightarrow a_4 \\rightarrow a_5 \\rightarrow a_6 \\rightarrow$\n",
    "Lose\n",
    "\n",
    "We can say all these actions are bad, and adjust our network to make sure it\n",
    "won't repeat these actions given the states it saw. We can do this by storing\n",
    "the sequence of actions in memory, then replay each of them, but associate them\n",
    "with a bad reward, so that the probability of each won't get repeated.\n",
    "\n",
    "However, we usually employ a more advanced approach where earlier actions in a\n",
    "sequence which led to a lose get punished more lightly than later actions. This\n",
    "makes sense since it's clear $a_6$ was a terrible action (it immediately led to\n",
    "our loss) but it's not so obvious $a_1$ was bad because the rest of the game\n",
    "might have unfolded differently.\n",
    "\n",
    "This is not a perfect solution. Some games with multiple phases struggle with\n",
    "this methodology, in general if it is not easy to get positive outcomes then our\n",
    "network will have trouble learning what actions are good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLssb8MT8oNf"
   },
   "source": [
    "### (Almost) Optimal Chess\n",
    "\n",
    "How do we put a good quality function together together? In the 1990s IBM Deep\n",
    "Blue used a handcrafted quality function, built with the input of several chess\n",
    "grand-masters. The function determined the quality of position by measuring\n",
    "things like how safe the King was or how easy it would be to set up a common\n",
    "attacking position. These values were fine tunned by watching the outcome of\n",
    "various games between other grand-masters.\n",
    "\n",
    "This wasn't very machine learning like, instead they just relied on human\n",
    "knowledge and the ability of the computer to evaluate over 200 million board\n",
    "positions a second. This type of strategy was enough to beat the world's best\n",
    "(and probably any subsequent humans).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoCxVeFl8oNg"
   },
   "source": [
    "### (Almost) Optimal Go\n",
    "\n",
    "Go is a much more complex game compared to chess. The problem is there are many\n",
    "more nodes we need to evaluate. Using a similar strategy to Deep Blue computer\n",
    "systems were able to defeat most amateur players, but could not compete with any\n",
    "professionals.\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1ssbb4keuNdpJb-Vry9IE5_AJ64ww8pSW\" width = 400>\n",
    "<center><i>(Image Source: <a>https://upload.wikimedia.org/wikipedia/commons/2/2a/FloorGoban.JPG</a>)</i></center>\n",
    "\n",
    "To beat Go players we need a better way of exploring the network and evaluating\n",
    "states of the game. To do this, Google Alpha Go trained in three stages.\n",
    "\n",
    "1. The network watched \"replays\" of expert Go players. It allowed it to learn\n",
    "   that given some state $S$, the correct action (class) was $a_i$.\n",
    "2. Once the network could copy humans players fairly well (it was making\n",
    "   somewhat calculated moves instead of random moves), it was cloned and allowed\n",
    "   to play against itself many times. Now it can play games and learn about the\n",
    "   rewards of potentially different game styles, and it can do so much faster\n",
    "   than any human could.\n",
    "3. Another network, a \"Value Network\" studies the potential victories and losses\n",
    "   from different states of the board as experienced by the Policy Network\n",
    "   playing against itself, and works to output a single Q value for a given\n",
    "   board. i.e. given a board state $S$, how good is that state? The use of that\n",
    "   network comes when AlphaGo wants to look at potential states several actions\n",
    "   in the future, but is able to quickly \"prune\" the search space by ignoring\n",
    "   paths that entail bad states.\n",
    "\n",
    "How we train these networks is pretty complex, and expensive. Some early\n",
    "versions of Alpha Go trained on over 1500 CPUs and 200 GPUs.\n",
    "\n",
    "Newer versions with smarter training and network structure, Alpha Zero, only\n",
    "require 4 TPUs to train and uses no human input. Within a few days Alpha Zero\n",
    "surpassed Alpha Go beating it 100 games to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/8tq1C8spV_g\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a29fe3c88>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.youtube.com/embed/8tq1C8spV_g\", width=560, height=315)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Reinforcement Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "09fa77dfce31738cd768fb92202d4b676dd1336a655854d9fb98a0feba06c452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
